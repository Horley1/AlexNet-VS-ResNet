{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftFwIprgv7mk"
      },
      "outputs": [],
      "source": [
        "%pip install torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import random\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from datasets import load_dataset, ClassLabel\n",
        "from torchvision import transforms\n",
        "from tqdm import tqdm\n",
        "from torchmetrics.classification import MulticlassF1Score\n",
        "\n",
        "########################################\n",
        "# 1. Фиксация seed\n",
        "########################################\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "    print(f\"[ OK ] global seed = {seed}\")\n",
        "\n",
        "seed = 42\n",
        "seed_everything(seed)\n",
        "\n",
        "# For DataLoader reproducibility\n",
        "def seed_worker(worker_id):\n",
        "    worker_seed = (seed + worker_id) % 2**32\n",
        "    np.random.seed(worker_seed)\n",
        "    random.seed(worker_seed)\n",
        "\n",
        "g = torch.Generator()\n",
        "g.manual_seed(seed)\n",
        "\n",
        "########################################\n",
        "# Dataset + transforms\n",
        "########################################\n",
        "tiny_imagenet = load_dataset('Maysee/tiny-imagenet')\n",
        "tiny_imagenet = tiny_imagenet.cast_column(\"label\", ClassLabel(num_classes=200))\n",
        "\n",
        "\n",
        "def train_transform_function(examples):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.ColorJitter(brightness=(0.9, 1.08), contrast=(0.9, 1.08)),\n",
        "        transforms.RandomResizedCrop(64, scale=(0.8, 0.95)),\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(\n",
        "            mean=[0.485, 0.456, 0.406],\n",
        "            std=[0.229, 0.224, 0.225]\n",
        "        )\n",
        "    ])\n",
        "\n",
        "    examples['image'] = [transform(img.convert(\"RGB\")) for img in examples['image']]\n",
        "    return examples\n",
        "\n",
        "\n",
        "def val_transform_function(examples):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(\n",
        "            mean=[0.485, 0.456, 0.406],\n",
        "            std=[0.229, 0.224, 0.225]\n",
        "        )\n",
        "    ])\n",
        "\n",
        "    examples['image'] = [transform(img.convert(\"RGB\")) for img in examples['image']]\n",
        "    return examples\n",
        "\n",
        "\n",
        "train_tiny_imagenet = tiny_imagenet['train'].with_transform(train_transform_function)\n",
        "val_tiny_imagenet = tiny_imagenet['valid'].with_transform(val_transform_function)\n",
        "\n",
        "\n",
        "########################################\n",
        "# DataLoaders with deterministic workers\n",
        "########################################\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_tiny_imagenet,\n",
        "    batch_size=128,\n",
        "    shuffle=True,\n",
        "    num_workers=28,\n",
        "    pin_memory=True,\n",
        "    worker_init_fn=seed_worker,\n",
        "    generator=g\n",
        ")\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_tiny_imagenet,\n",
        "    batch_size=128,\n",
        "    shuffle=False,\n",
        "    num_workers=28,\n",
        "    pin_memory=True,\n",
        "    worker_init_fn=seed_worker,\n",
        "    generator=g\n",
        ")\n",
        "\n",
        "########################################\n",
        "# Device\n",
        "########################################\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "00iKGvGBwCDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def testing(model_name='alexnet',\n",
        "            l_rate=1e-4,\n",
        "            w_decay=5e-5,\n",
        "            log_file='training_log.txt'):\n",
        "\n",
        "    original_stdout = sys.stdout\n",
        "    with open(log_file, 'a') as f:\n",
        "        sys.stdout = f\n",
        "\n",
        "        print(f\"\\n\\n\\nlr = {l_rate}, weight_decay = {w_decay}\")\n",
        "\n",
        "        if model_name == 'alexnet':\n",
        "            model = torchvision.models.alexnet()  #weights='DEFAULT'\n",
        "            model.classifier[6] = nn.Linear(4096, 200)\n",
        "        else:\n",
        "            model = torchvision.models.resnet18()   #weights='DEFAULT'\n",
        "            model.fc = nn.Linear(512, 200)\n",
        "\n",
        "        model = model.to(DEVICE)\n",
        "\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = torch.optim.SGD(model.parameters(), lr=l_rate, momentum=0.9, weight_decay=w_decay)\n",
        "        f1_metric = MulticlassF1Score(num_classes=200, average=\"macro\").to(DEVICE)\n",
        "\n",
        "        n_epochs = 80\n",
        "\n",
        "        for epoch in range(n_epochs):\n",
        "            model.train()\n",
        "\n",
        "            train_loss = 0.0\n",
        "            train_acc = 0.0\n",
        "            f1_metric.reset()\n",
        "\n",
        "            for batch in tqdm(train_loader):\n",
        "                data = batch['image'].to(DEVICE)\n",
        "                target = batch['label'].to(DEVICE)\n",
        "                optimizer.zero_grad()\n",
        "                output = model(data)\n",
        "                loss = criterion(output, target)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                train_loss += loss.item() * data.size(0)\n",
        "                train_acc += (output.argmax(1) == target).sum().item()\n",
        "\n",
        "            train_loss = train_loss / len(train_loader.dataset)\n",
        "            train_acc = train_acc / len(train_loader.dataset)\n",
        "\n",
        "            model.eval()\n",
        "\n",
        "            val_acc = 0.0\n",
        "            val_acc_top5 = 0.0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for batch in tqdm(val_loader):\n",
        "                    data = batch['image'].to(DEVICE)\n",
        "                    target = batch['label'].to(DEVICE)\n",
        "                    output = model(data)\n",
        "                    val_acc += (output.argmax(1) == target).sum().item()\n",
        "                    _, pred_top5 = output.topk(5, dim=1)\n",
        "                    val_acc_top5 += sum(target[i].item() in pred_top5[i] for i in range(target.size(0)))\n",
        "                    f1_metric.update(output, target)\n",
        "\n",
        "\n",
        "            val_acc = val_acc / len(val_loader.dataset)\n",
        "            val_acc_top5 /= len(val_loader.dataset)\n",
        "            val_f1 = f1_metric.compute().item()\n",
        "\n",
        "\n",
        "            print(\n",
        "                f\"Epoch: {epoch+1} \"\n",
        "                f\"Train Loss: {train_loss:.6f} \"\n",
        "                f\"Train Acc: {train_acc:.6f} \"\n",
        "                f\"Val Top1: {val_acc:.6f} \"\n",
        "                f\"Val Top5: {val_acc_top5:.6f} \"\n",
        "                f\"F1: {val_f1:.6f} \"\n",
        "            )\n",
        "\n",
        "        sys.stdout = original_stdout"
      ],
      "metadata": {
        "id": "Nz9Mh5GawE2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for lr in [1e-5, 1e-4, 1e-3, 1e-2]:\n",
        "    for wd in [5e-5, 5e-4. 5e-3, 5e-2]:\n",
        "        testing(model_name='alexnet',\n",
        "                l_rate=lr,\n",
        "                w_decay=wd,\n",
        "                log_file='AlexNet_without_pretrain_upd.txt'\n",
        "                )\n",
        "\n",
        "for lr in [1e-5, 1e-4, 1e-3, 1e-2]:\n",
        "    for wd in [5e-5, 5e-4, 5e-3, 5e-2]:\n",
        "        testing(model_name='resnet18',\n",
        "                l_rate=lr,\n",
        "                w_decay=wd,\n",
        "                log_file='ResNet_without_pretrain_upd.txt'\n",
        "                )"
      ],
      "metadata": {
        "id": "CsbFV_UJwKLn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}